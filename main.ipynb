# -*- coding: utf-8 -*-
"""Copy_of_Llama_3_1_8b_+_Unsloth_2x_faster_finetuning (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18eT5pkRYrK7qCujlsyerz-R8l-8tATSg
"""

from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# # Also get the latest nightly Unsloth!
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

!pip install huggingface_hub

from huggingface_hub import login

login(
  token="", # ADD YOUR TOKEN HERE
  add_to_git_credential=True
)

from unsloth import FastLanguageModel
import torch
max_seq_length = 300 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 15 trillion tokens model 2x faster!
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # We also uploaded 4bit for 405b!
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # New Mistral 12b 2x faster!
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!
] # More models at https://huggingface.co/unsloth

latest_checkpoint = "/content/drive/MyDrive/protein-generator-seq2seq"

model, tokenizer = FastLanguageModel.from_pretrained(
    # model_name = "unsloth/Meta-Llama-3.1-8B",
    model_name = latest_checkpoint,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "v_proj"],
    lora_alpha = 16,
    lora_dropout = 0.2, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

# import json
# from datasets import load_dataset, Dataset
# from sklearn.model_selection import train_test_split
# import os

# # Load dataset from file
# file_path = '/content/drive/MyDrive/protein-dataset/protein_design.json'

# with open(file_path, 'r') as f:
#     # Check if the file exists and is not empty
#     if os.path.getsize(file_path) > 0:
#         print("File found and loaded.")
#         data = json.load(f)
#     else:
#         raise ValueError("File is empty or not found!")

# # Convert list of dictionaries to Hugging Face Dataset
# dataset = Dataset.from_list(data)

# # Shuffle the dataset
# shuffled_dataset = dataset.shuffle(seed=42)

# Function to randomly replace instructions for uncontrollable generation
def randomize_instructions(examples, uncontrollable_ratio=0.3):
    for i in range(len(examples['instruction'])):
        if random.random() < uncontrollable_ratio:
          # for random proteins we don't need input
            examples['input'][i] = ""
            examples['instruction'][i] = "Generate a random protein sequence."
    return examples

import logging
import json
from datasets import Dataset, concatenate_datasets
import random
from typing import Dict, Any, List

def normalize_metadata(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalizes metadata structure by ensuring all required fields are present
    with default values if missing.
    """
    # Define default metadata structure
    default_metadata = {
        'num_annots': 0,
        'num_tokens_in_input': 0,
        'protein_accession': '',
        'seq_len': 0,
        'split': 'train',
        'task': '',
        'annots': ''  # Add default value for annots
    }

    # Get existing metadata
    metadata = item.get('metadata', {})

    # Update with defaults for missing fields
    for key, default_value in default_metadata.items():
        if key not in metadata:
            metadata[key] = default_value

    return {
        'instruction': item.get('instruction', ''),
        'input': item.get('input', ''),
        'output': item.get('output', ''),
        'metadata': metadata
    }

def load_and_merge_datasets(
    dataset1_path: str,
    dataset2_path: str,
    randomize: bool = True,
    uncontrollable_ratio: float = 0.3
) -> Dataset:
    """
    Loads two JSON datasets, normalizes their schemas, and merges them.

    Args:
        dataset1_path (str): Path to first JSON dataset
        dataset2_path (str): Path to second JSON dataset
        randomize (bool): Whether to apply randomization
        uncontrollable_ratio (float): Ratio for instruction randomization

    Returns:
        Dataset: Merged dataset with normalized schema
    """
    logging.info(f"Loading datasets from {dataset1_path} and {dataset2_path}")

    try:
        # Load JSON datasets
        with open(dataset1_path, 'r', encoding='utf-8') as f:
            data1 = json.load(f)
        with open(dataset2_path, 'r', encoding='utf-8') as f:
            data2 = json.load(f)

        # Normalize data structures
        if isinstance(data1, list):
            normalized_data1 = [normalize_metadata(item) for item in data1]
        else:
            raise ValueError("Dataset 1 must be a list of dictionaries")

        if isinstance(data2, list):
            normalized_data2 = [normalize_metadata(item) for item in data2]
        else:
            raise ValueError("Dataset 2 must be a list of dictionaries")

        # Convert to Hugging Face datasets
        dataset1 = Dataset.from_list(normalized_data1)
        dataset2 = Dataset.from_list(normalized_data2)

        # Apply randomization if requested
        if randomize and 'randomize_instructions' in globals():
            logging.info(f"Applying randomization with ratio {uncontrollable_ratio}")
            dataset1 = dataset1.map(
                lambda x: randomize_instructions(x, uncontrollable_ratio),
                batched=True,
                desc="Randomizing instructions"
            )

        # Merge datasets
        print(dataset1[1])
        print(dataset2[1])
        merged_dataset = concatenate_datasets([dataset1, dataset2])

        logging.info(f"Successfully merged datasets. Total size: {len(merged_dataset)}")
        return merged_dataset

    except Exception as e:
        logging.error(f"Error merging datasets: {str(e)}")
        raise

def print_dataset_info(dataset: Dataset):
    """
    Prints information about the dataset structure and contents.
    """
    print("\nDataset Info:")
    print(f"Number of examples: {len(dataset)}")
    print("\nFeatures:", dataset.features)
    print("\nFirst example:")
    print(json.dumps(dataset[0], indent=2))

# Example usage:
if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(level=logging.INFO)

    # Load and merge datasets
    try:
        merged_dataset = load_and_merge_datasets(
            "protein_design.json",
            "protein_function.json"
        )
        print_dataset_info(merged_dataset)
    except Exception as e:
        logging.error(f"Failed to merge datasets: {str(e)}")

# import json
# from datasets import load_dataset, Dataset
# from sklearn.model_selection import train_test_split
# import os

# # merge datasets
file_path = '/content/drive/MyDrive/protein-dataset/protein_design.json'
file_path2 = '/content/drive/MyDrive/protein-dataset/protein_function.json'
merged_dataset = load_and_merge_datasets(file_path, file_path2)

# prompt: write a code to show 3 table in pandas: protein_design, protein_function and merge of them

import pandas as pd

# Load the datasets from JSON files (replace with your actual file paths)
protein_design_df = pd.read_json('/content/drive/MyDrive/protein-dataset/protein_design.json')
protein_function_df = pd.read_json('/content/drive/MyDrive/protein-dataset/protein_function.json')


# Display the protein_design DataFrame
print("Protein Design:")
# print(protein_design_df)
protein_design_df

# Display the protein_function DataFrame
# print("\nProtein Function:")
# protein_function_df

# Assuming there's a common column for merging, such as "protein_id" or similar
# Replace 'common_column' with the actual name of the shared column
# if 'common_column' not in protein_design_df.columns or 'common_column' not in protein_function_df.columns:
#   raise ValueError("The specified common column does not exist in both DataFrames.")

# Merge the two DataFrames
# merged_df = pd.merge(protein_design_df, protein_function_df, on='common_column', how='inner') # or how='outer', 'left', 'right'

# Display the merged DataFrame
# print("\nMerged DataFrame:")
# print(merged_df)

# If there is no common column, provide an alternative merging strategy or just display dataframes as above.



merged_dataset = load_and_merge_datasets(file_path, file_path2)

print(merged_dataset[0])

# Convert list of dictionaries to Hugging Face Dataset
dataset = Dataset.from_list(merged_dataset)

# Shuffle the dataset
shuffled_dataset = dataset.shuffle(seed=42)

print(shuffled_dataset[2])
print(next(i for i, item in enumerate(shuffled_dataset) if item["metadata"]["task"] == "GO"))

import random
from datasets import Dataset
from sklearn.model_selection import train_test_split
import re

# Split dataset into train and test sets
split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)

train_dataset = split_dataset['train']
test_dataset = split_dataset['test']

# Rebalance data by trimming long instructions or augmenting short ones
def balance_data(examples):
    max_tokens = 300  # Adjust this as per your needs
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]

    balanced_instructions = []
    balanced_inputs = []
    balanced_outputs = []

    for inst, inp, outp in zip(instructions, inputs, outputs):
        if len(inst.split()) <= max_tokens:  # Keep examples with a reasonable number of tokens
            balanced_instructions.append(inst)
            balanced_inputs.append(inp)
            balanced_outputs.append(outp)

    return {"instruction": balanced_instructions, "input": balanced_inputs, "output": balanced_outputs}

# Apply the balance_data function
train_dataset = train_dataset.map(balance_data, batched=True)

# read train and test dataset from google drive
from datasets import load_from_disk
train_dataset = load_from_disk("/content/drive/MyDrive/protein-dataset/train_dataset")
test_dataset = load_from_disk("/content/drive/MyDrive/protein-dataset/test_dataset")

print(test_dataset[1])
print(train_dataset[1])

# empty cache
torch.cuda.empty_cache()

def format_input_output(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    metadatas = examples["metadata"]

    inputs_formatted = []
    outputs_formatted = []

    for instruction, input_text, output, metadata in zip(instructions, inputs, outputs, metadatas):

        protein_accession = metadata.get("protein_accession", "Unknown")
        seq_len = metadata.get("seq_len", "Unknown")
        task = metadata.get("task", "Unknown")
        num_tokens_in_input = metadata.get("num_tokens_in_input", "Unknown")

        formatted_input = (
            f"### Instruction:\n{instruction}\n\n"
            f"### Input:\n{input_text}\n"
            f"### Metadata:\nprotein_accession: {protein_accession}, sequence_length: {seq_len}, task: {task}, num_tokens_in_input: {num_tokens_in_input}\n"

        )
        formatted_output = f"{output}"
        inputs_formatted.append(formatted_input)
        outputs_formatted.append(formatted_output)

    return {"input_text": inputs_formatted, "output_text": outputs_formatted}

split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)

train_dataset = split_dataset['train']
test_dataset = split_dataset['test']

train_dataset = train_dataset.map(format_input_output, batched=True)
test_dataset = test_dataset.map(format_input_output, batched=True)

print(train_dataset[1])
print(next(i for i, item in enumerate(train_dataset) if item["metadata"]["task"] == "GO"))

# save train and test datasets in google drive
train_dataset.save_to_disk("/content/drive/MyDrive/protein-dataset/train_dataset")
test_dataset.save_to_disk("/content/drive/MyDrive/protein-dataset/test_dataset")

print(train_dataset[1])
print(next(i for i, item in enumerate(train_dataset) if item["metadata"]["task"] == "GO"))

# def tokenize_function(examples):
#     model_inputs = tokenizer(examples["input_text"], padding="max_length", truncation=True, max_length=512)
#     labels = tokenizer(examples["output_text"], padding="max_length", truncation=True, max_length=512)
#     model_inputs["labels"] = labels["input_ids"]  # Shift labels for decoder
#     return model_inputs

# # Apply tokenization
# train_dataset = train_dataset.map(tokenize_function, batched=True)
# test_dataset = test_dataset.map(tokenize_function, batched=True)



def tokenize_function(examples):
    model_inputs = tokenizer(examples["input_text"], padding="max_length", truncation=True, max_length=1024)
    labels = tokenizer(examples["output_text"], padding="max_length", truncation=True, max_length=1024)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


train_dataset = train_dataset.remove_columns(["instruction", "input", "output", "metadata"])
test_dataset = test_dataset.remove_columns(["instruction", "input", "output", "metadata"])

# Apply tokenization across the dataset
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

print(train_dataset[0])

# # Check the data type of fields in a sample row
# for i in range(3):  # Checking the first 3 rows
#     print(f"Row {i}:")
#     print("Type of input_text:", type(train_dataset[i]["input_text"]))
#     print("Type of output_text:", type(train_dataset[i]["output_text"]))
#     print("input_text:", train_dataset[i]["input_text"])
#     print("output_text:", train_dataset[i]["output_text"])
#     print("----")


# single_row = train_dataset[0]
# model_inputs = tokenizer(single_row["input_text"], padding="max_length", truncation=True, max_length=512)
# labels = tokenizer(single_row["output_text"], padding="max_length", truncation=True, max_length=512)

# print("Tokenized input_text:", model_inputs)
# print("Tokenized output_text:", labels)

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-2,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=1,
    save_steps=1000,
    save_total_limit=3,
    load_best_model_at_end=True,
    warmup_steps=500,
    lr_scheduler_type="cosine",
    max_grad_norm=0.5,
    # Set save_strategy to match evaluation_strategy
    save_strategy="epoch",
    # remove_unused_columns=False
)

# Initialize Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Add EarlyStoppingCallback
)

# Train the model
trainer.train()
# c2eddab67187aa35b1dd2aba40f692c6289604e4

torch.cuda.empty_cache()

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,  # Can make training faster for short sequences.
    args=TrainingArguments(
        per_device_train_batch_size=32,
        gradient_accumulation_steps=8,  # Reduced to update weights more frequently.
        warmup_steps=100,  # Increased to better adjust learning rate initially.
        num_train_epochs=5,  # Increased to give the model more training iterations.
        max_steps=-1,  # Increased to ensure sufficient training time.
        learning_rate=1e-4,  # Slightly reduced for stability with the larger effective batch size.
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,  # Reduced logging frequency to decrease verbosity.
        optim="adamw_8bit",
        max_grad_norm=1.0,
        weight_decay=0.1,
        lr_scheduler_type="cosine",
        seed=3407,
        output_dir="/content/drive/MyDrive/model_unsloth_outputs",  # Save directly to Google Drive
        save_strategy="steps",
        save_steps=30,
        save_total_limit=1
    ),
)

#@title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()
# c2eddab67187aa35b1dd2aba40f692c6289604e4

print(trainer_stats)

# prompt: create chart of train loss

import matplotlib.pyplot as plt

# Assuming trainer_stats contains the training loss history
train_loss = trainer_stats.training_loss

# Create a plot of the training loss
plt.plot(train_loss)
plt.xlabel("Training Steps")
plt.ylabel("Training Loss")
plt.title("Training Loss over Time")
plt.show()

model.push_to_hub("", token="")
tokenizer.push_to_hub("", token="")

save_directory = '/content/drive/MyDrive/protein-generator-seq2seq'

# Save the model
model.save_pretrained(save_directory)

# Save the tokenizer
tokenizer.save_pretrained(save_directory)

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from unsloth import FastLanguageModel


max_seq_length = 300 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True
# latest_checkpoint = "/content/drive/MyDrive/model_unsloth_outputs/checkpoint-125"
latest_checkpoint = "/content/drive/MyDrive/protein-generator-seq2seq"


model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = latest_checkpoint,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "v_proj"],
    lora_alpha = 16,
    lora_dropout = 0.2, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

# generate uncontrollable proteins using the model

pip install -U bitsandbytes

# generate uncontrollable proteins using the model

"""# Controllable Protein Generation"""

def generate_protein(prompt):
    FastLanguageModel.for_inference(model)

    # First, format the prompt
    # formatted_prompt = f"Instruction: {prompt}\nOutput: Generate a protein sequence"
    formatted_prompt = f"Instruction:\nGenerate a protein sequence"

    # Then, tokenize the formatted prompt
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

    # Generate the output
    # model.bfloat16()
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7, top_p=0.9)

    # Decode the output
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated_text

# Example prompt for protein design
prompt = "Design a protein sequence that binds to a specific target molecule involved in cell growth and differentiation."

# Generate protein sequence
generated_protein = generate_protein(prompt)

print(f"Generated protein sequence:\n{generated_protein}")

"""# Uncontrollable Protein Generation"""

import re
import torch

FastLanguageModel.for_inference(model)

# Define a function to clean the generated protein sequence
def extract_protein_sequence(text):
    # Only keep valid amino acid characters
    return re.sub(r'[^ACDEFGHIKLMNPQRSTVWY]', '', text)

# Define the protein generation function
def generate_uncontrollable_proteins(model, tokenizer, num_proteins=3, max_length=300):
    generated_proteins = []
    model.eval()  # Ensure the model is in inference mode

    for _ in range(num_proteins):
        # Define the input instruction
        input_instruction = "Generate only the protein sequence without any additional information."
        input_tokens = tokenizer(input_instruction, return_tensors="pt", padding=True, truncation=True)

        # Generate the protein sequence
        with torch.no_grad():  # Disable gradient computation for inference
            generated_ids = model.generate(
                input_ids=input_tokens['input_ids'],
                attention_mask=input_tokens['attention_mask'],
                max_length=max_length,
                # num_beams=5,            # Optional: Use beam search for better quality
                early_stopping=True     # Stop when EOS token is generated
            )

        # Decode and clean up the generated sequence
        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        # clean_protein_sequence = extract_protein_sequence(generated_text)

        generated_proteins.append(generated_text)

    return generated_proteins

# Example usage
generated_proteins = generate_uncontrollable_proteins(model, tokenizer)
print(generated_proteins)

import re

def clean_generated_text(text, prompt):
    """
    Removes prompt repetitions and irrelevant text from generated output.
    """
    # Remove repetitive prompts or identical lines
    unique_lines = []
    for line in text.splitlines():
        if line not in unique_lines and line.strip() != prompt.strip():
            unique_lines.append(line.strip())
    return "\n".join(unique_lines)

def predict_protein_function(model, tokenizer, protein_sequence):
    """
    Predicts the function of a given protein sequence.
    """
    FastLanguageModel.for_inference(model)

    # Define input prompt with a specific instruction
    input_prompt = f"Predict the biological function of this protein sequence:\n{protein_sequence}\n\nFunction:"

    # Tokenize and encode the prompt for the model
    input_text = tokenizer(input_prompt, return_tensors="pt", padding=True, truncation=True)

    # Generate the function prediction with refined parameters
    generated_text = model.generate(
        input_text['input_ids'],
        max_length=150,  # Limit output length
        # num_beams=3,     # Beam search for varied outputs
        no_repeat_ngram_size=3,  # Avoid repetition of trigrams
        top_p=0.9,       # Top-p sampling for more diverse output
        temperature=0.8, # Add slight randomness for variety
        early_stopping=True
    )

    # Decode and clean generated output
    function_description = tokenizer.decode(generated_text[0], skip_special_tokens=True)
    cleaned_description = clean_generated_text(function_description, input_prompt)

    return cleaned_description

# Example usage
protein_sequence = "MKTIIALSYIFCLVFADYKDDDDK"
function_prediction = predict_protein_function(model, tokenizer, protein_sequence)
print("Predicted Function:", function_prediction)

# def predict_protein_function(sequence, model, tokenizer, go_terms, unique_terms):
#     try:
#         # Tokenize the input sequence
#         print(f"Tokenizing sequence: {sequence}")
#         inputs = tokenizer(sequence, return_tensors="pt", padding=True, truncation=True, max_length=1022)
#         print(f"Tokenized inputs: {inputs}")
#         model.eval()

#         # Make predictions
#         with torch.no_grad():
#             outputs = model(**inputs)
#             print(f"Model outputs: {outputs}")
#             if outputs.logits is None:
#                 print("Model did not return logits.")
#                 return []
#             predictions = torch.sigmoid(outputs.logits)
#             print(f"Predictions: {predictions}")
#             predicted_indices = torch.where(predictions > 0.05)[1].tolist()
#             print(f"Predicted indices: {predicted_indices}")

#         functions = []
#         for idx in predicted_indices:
#             term_id = unique_terms[idx]  # Use the unique_terms list from your training script
#             for term in go_terms:
#                 if term["id"] == term_id:
#                     functions.append(term["name"])
#                     break

#         return functions

#     except Exception as e:
#         print(f"Error in predict_protein_function: {e}")
#         return []


# def parse_obo_file(file_path):
#     with open(file_path, 'r') as f:
#         data = f.read().split("[Term]")

#     terms = []
#     for entry in data[1:]:
#         lines = entry.strip().split("\n")
#         term = {}
#         for line in lines:
#             if line.startswith("id:"):
#                 term["id"] = line.split("id:")[1].strip()
#             elif line.startswith("name:"):
#                 term["name"] = line.split("name:")[1].strip()
#             elif line.startswith("namespace:"):
#                 term["namespace"] = line.split("namespace:")[1].strip()
#             elif line.startswith("def:"):
#                 term["definition"] = line.split("def:")[1].split('"')[1]
#         terms.append(term)
#     return terms

# from transformers import AutoTokenizer, AutoModelForSequenceClassification, EsmForSequenceClassification
# import torch

# go_basic_path = "/content/drive/MyDrive/Colab Notebooks/data/go-basic.obo"
# parsed_terms = parse_obo_file(go_basic_path)
# print(f"Parsed GO terms: {parsed_terms[:5]}")  # Print first 5 terms for debugging

# # Extract unique terms from the GO terms
# unique_terms = [term["id"] for term in parsed_terms]
# print(f"Unique terms: {unique_terms[:5]}")  # Print first 5 unique terms for debugging\\


# protein_sequence = "MIVLITSLTAKKLKSTAVVLLFQFQLSRLFVLSSVGGVFLAVHAKAFLVSSGKVLVQYLSLQGAVVLLTQVSTLGLLLSGGAKALVVLTAVGQKLGKLLTLVAVLHLLHFLKLLTLLKTLTAVVLLTLLGTVLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLL"
# prediction_model_name = "AmelieSchreiber/cafa_5_protein_function_prediction"  # Update this with your actual model name
# tokenizer = AutoTokenizer.from_pretrained(prediction_model_name)
# model = EsmForSequenceClassification.from_pretrained(prediction_model_name)
# # input = tokenizer(protein_sequence, return_tensors="pt", padding=True, truncation=True)
# # outputs = model(**input)
# predict_protein_function(protein_sequence, model, tokenizer, parsed_terms, unique_terms)

!pip install goatools
from transformers import BertTokenizer, BertModel
import torch
from torch import nn
from goatools.obo_parser import GODag

# Function to parse the GO terms from the go-basic.obo file
def parse_obo_file(obo_path):
    go = GODag(obo_path)
    parsed_terms = []
    for go_term in go:
        term_info = {
            "id": go[go_term].id,
            "name": go[go_term].name,
            "definition": go[go_term].__dict__.get('definition', "No definition available")  # Use .get() to handle missing definitions
        }
        parsed_terms.append(term_info)
    return parsed_terms

# Function to predict the protein function using ProtBERT embeddings and a classifier
def predict_protein_function(protein_sequence, model, classifier, tokenizer, parsed_terms, unique_terms):
    # Preprocess and tokenize the input sequence
    input_sequence = " ".join(protein_sequence)  # Add spaces between amino acids for ProtBERT
    inputs = tokenizer(input_sequence, return_tensors="pt")

    # Forward pass through ProtBERT to get embeddings
    with torch.no_grad():
        outputs = model(**inputs)

    # The embeddings are found in the last hidden state
    embeddings = outputs.last_hidden_state

    # Take the mean of the token embeddings (global pooling) to get a single vector for classification
    protein_embedding = torch.mean(embeddings, dim=1)

    # Use the classifier to predict the GO term index
    output = classifier(protein_embedding)
    predicted_class_idx = torch.argmax(output, dim=1).item()

    # Retrieve the corresponding GO term using the class index
    predicted_go_term = unique_terms[predicted_class_idx]
    predicted_term_info = next((term for term in parsed_terms if term["id"] == predicted_go_term), None)

    if predicted_term_info:
        print(f"Predicted GO Term: {predicted_term_info['id']}")
        print(f"GO Term Name: {predicted_term_info['name']}")
        print(f"GO Term Definition: {predicted_term_info['definition']}")
    else:
        print(f"Predicted GO Term: {predicted_go_term} (No additional information found)")

# Path to the GO basic ontology file
go_basic_path = "/content/drive/MyDrive/Colab Notebooks/data/go-basic.obo"

# Parse the GO terms from the obo file
parsed_terms = parse_obo_file(go_basic_path)
print(f"Parsed GO terms: {parsed_terms[:5]}")  # Print first 5 terms for debugging

# Extract unique terms from the GO terms
unique_terms = [term["id"] for term in parsed_terms]
print(f"Unique terms: {unique_terms[:5]}")  # Print first 5 unique terms for debugging

# Example protein sequence
protein_sequence = (
    "MIVLITSLTAKKLKSTAVVLLFQFQLSRLFVLSSGAVVFLAVHAKAFLVSSGKVLVQYLSLQGAVVLLTQVSTLGLLLSGGAKALVVLTAVGQKLGKLLTLV"
    "AVLHLLHFLKLLTLLKTLTAVVLLTLLGTVLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLLTLL"
)

# Load ProtBERT tokenizer and model
prot_bert_model_name = "Rostlab/prot_bert"
tokenizer = BertTokenizer.from_pretrained(prot_bert_model_name)
prot_bert_model = BertModel.from_pretrained(prot_bert_model_name)

# Simple classifier for GO term prediction (you can replace this with your own trained classifier)
class ProteinGOClassifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(ProteinGOClassifier, self).__init__()
        self.fc = nn.Linear(input_size, num_classes)

    def forward(self, x):
        return self.fc(x)

# Assuming ProtBERT outputs 1024-dimensional embeddings, and you have N GO terms
num_go_terms = len(unique_terms)  # The number of unique GO terms
classifier = ProteinGOClassifier(input_size=1024, num_classes=num_go_terms)

# Example: Call the prediction function
predict_protein_function(protein_sequence, prot_bert_model, classifier, tokenizer, parsed_terms, unique_terms)

#@title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")



"""<img src="https://raw.githubusercontent.com/sokrypton/ColabFold/main/.github/ColabFold_Marv_Logo_Small.png" height="200" align="right" style="height:240px">

##ColabFold v1.5.5: AlphaFold2 using MMseqs2

Easy to use protein structure and complex prediction using [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) and [Alphafold2-multimer](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1). Sequence alignments/templates are generated through [MMseqs2](mmseqs.com) and [HHsearch](https://github.com/soedinglab/hh-suite). For more details, see <a href="#Instructions">bottom</a> of the notebook, checkout the [ColabFold GitHub](https://github.com/sokrypton/ColabFold) and [Nature Protocols](https://www.nature.com/articles/s41596-024-01060-5).

Old versions: [v1.4](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.4.0/AlphaFold2.ipynb), [v1.5.1](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.1/AlphaFold2.ipynb), [v1.5.2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.2/AlphaFold2.ipynb), [v1.5.3-patch](https://colab.research.google.com/github/sokrypton/ColabFold/blob/56c72044c7d51a311ca99b953a71e552fdc042e1/AlphaFold2.ipynb)

[Mirdita M, SchÃ¼tze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. ColabFold: Making protein folding accessible to all.
*Nature Methods*, 2022](https://www.nature.com/articles/s41592-022-01488-1)
"""

#@title Input protein sequence(s), then hit `Runtime` -> `Run all`
from google.colab import files
import os
import re
import hashlib
import random

from sys import version_info
python_version = f"{version_info.major}.{version_info.minor}"

def add_hash(x,y):
  return x+"_"+hashlib.sha1(y.encode()).hexdigest()[:5]

query_sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK' #@param {type:"string"}
#@markdown  - Use `:` to specify inter-protein chainbreaks for **modeling complexes** (supports homo- and hetro-oligomers). For example **PI...SK:PI...SK** for a homodimer
jobname = 'test' #@param {type:"string"}
# number of models to use
num_relax = 0 #@param [0, 1, 5] {type:"raw"}
#@markdown - specify how many of the top ranked structures to relax using amber
template_mode = "none" #@param ["none", "pdb100","custom"]
#@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))

use_amber = num_relax > 0

# remove whitespaces
query_sequence = "".join(query_sequence.split())

basejobname = "".join(jobname.split())
basejobname = re.sub(r'\W+', '', basejobname)
jobname = add_hash(basejobname, query_sequence)

# check if directory with jobname exists
def check(folder):
  if os.path.exists(folder):
    return False
  else:
    return True
if not check(jobname):
  n = 0
  while not check(f"{jobname}_{n}"): n += 1
  jobname = f"{jobname}_{n}"

# make directory to save results
os.makedirs(jobname, exist_ok=True)

# save queries
queries_path = os.path.join(jobname, f"{jobname}.csv")
with open(queries_path, "w") as text_file:
  text_file.write(f"id,sequence\n{jobname},{query_sequence}")

if template_mode == "pdb100":
  use_templates = True
  custom_template_path = None
elif template_mode == "custom":
  custom_template_path = os.path.join(jobname,f"template")
  os.makedirs(custom_template_path, exist_ok=True)
  uploaded = files.upload()
  use_templates = True
  for fn in uploaded.keys():
    os.rename(fn,os.path.join(custom_template_path,fn))
else:
  custom_template_path = None
  use_templates = False

print("jobname",jobname)
print("sequence",query_sequence)
print("length",len(query_sequence.replace(":","")))

# Commented out IPython magic to ensure Python compatibility.
# #@title Install dependencies
# %%time
# import os
# USE_AMBER = use_amber
# USE_TEMPLATES = use_templates
# PYTHON_VERSION = python_version
# 
# if not os.path.isfile("COLABFOLD_READY"):
#   print("installing colabfold...")
#   os.system("pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'")
#   if os.environ.get('TPU_NAME', False) != False:
#     os.system("pip uninstall -y jax jaxlib")
#     os.system("pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html")
#   os.system("ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold")
#   os.system("ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold")
#   os.system("touch COLABFOLD_READY")
# 
# if USE_AMBER or USE_TEMPLATES:
#   if not os.path.isfile("CONDA_READY"):
#     print("installing conda...")
#     os.system("wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh")
#     os.system("bash Miniforge3-Linux-x86_64.sh -bfp /usr/local")
#     os.system("mamba config --set auto_update_conda false")
#     os.system("touch CONDA_READY")
# 
# if USE_TEMPLATES and not os.path.isfile("HH_READY") and USE_AMBER and not os.path.isfile("AMBER_READY"):
#   print("installing hhsuite and amber...")
#   os.system(f"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer")
#   os.system("touch HH_READY")
#   os.system("touch AMBER_READY")
# else:
#   if USE_TEMPLATES and not os.path.isfile("HH_READY"):
#     print("installing hhsuite...")
#     os.system(f"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'")
#     os.system("touch HH_READY")
#   if USE_AMBER and not os.path.isfile("AMBER_READY"):
#     print("installing amber...")
#     os.system(f"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer")
#     os.system("touch AMBER_READY")

#@markdown ### MSA options (custom MSA upload, single sequence, pairing mode)
msa_mode = "mmseqs2_uniref_env" #@param ["mmseqs2_uniref_env", "mmseqs2_uniref","single_sequence","custom"]
pair_mode = "unpaired_paired" #@param ["unpaired_paired","paired","unpaired"] {type:"string"}
#@markdown - "unpaired_paired" = pair sequences from same species + unpaired MSA, "unpaired" = seperate MSA for each chain, "paired" - only use paired sequences.

# decide which a3m to use
if "mmseqs2" in msa_mode:
  a3m_file = os.path.join(jobname,f"{jobname}.a3m")

elif msa_mode == "custom":
  a3m_file = os.path.join(jobname,f"{jobname}.custom.a3m")
  if not os.path.isfile(a3m_file):
    custom_msa_dict = files.upload()
    custom_msa = list(custom_msa_dict.keys())[0]
    header = 0
    import fileinput
    for line in fileinput.FileInput(custom_msa,inplace=1):
      if line.startswith(">"):
         header = header + 1
      if not line.rstrip():
        continue
      if line.startswith(">") == False and header == 1:
         query_sequence = line.rstrip()
      print(line, end='')

    os.rename(custom_msa, a3m_file)
    queries_path=a3m_file
    print(f"moving {custom_msa} to {a3m_file}")

else:
  a3m_file = os.path.join(jobname,f"{jobname}.single_sequence.a3m")
  with open(a3m_file, "w") as text_file:
    text_file.write(">1\n%s" % query_sequence)

#@markdown ### Advanced settings
model_type = "auto" #@param ["auto", "alphafold2_ptm", "alphafold2_multimer_v1", "alphafold2_multimer_v2", "alphafold2_multimer_v3", "deepfold_v1", "alphafold2"]
#@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.
#@markdown Any of the mode_types can be used (regardless if input is monomer or complex).
num_recycles = "3" #@param ["auto", "0", "1", "3", "6", "12", "24", "48"]
#@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .
recycle_early_stop_tolerance = "auto" #@param ["auto", "0.0", "0.5", "1.0"]
#@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.
relax_max_iterations = 200 #@param [0, 200, 2000] {type:"raw"}
#@markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)
pairing_strategy = "greedy" #@param ["greedy", "complete"] {type:"string"}
#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.
calc_extra_ptm = False #@param {type:"boolean"}
#@markdown - return pairwise chain iptm/actifptm

#@markdown #### Sample settings
#@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.
#@markdown -  decrease `max_msa` to increase uncertainity
max_msa = "auto" #@param ["auto", "512:1024", "256:512", "64:128", "32:64", "16:32"]
num_seeds = 1 #@param [1,2,4,8,16] {type:"raw"}
use_dropout = False #@param {type:"boolean"}

num_recycles = None if num_recycles == "auto" else int(num_recycles)
recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == "auto" else float(recycle_early_stop_tolerance)
if max_msa == "auto": max_msa = None

#@markdown #### Save settings
save_all = False #@param {type:"boolean"}
save_recycles = False #@param {type:"boolean"}
save_to_google_drive = False #@param {type:"boolean"}
#@markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive
dpi = 200 #@param {type:"integer"}
#@markdown - set dpi for image resolution

if save_to_google_drive:
  from pydrive2.drive import GoogleDrive
  from pydrive2.auth import GoogleAuth
  from google.colab import auth
  from oauth2client.client import GoogleCredentials
  auth.authenticate_user()
  gauth = GoogleAuth()
  gauth.credentials = GoogleCredentials.get_application_default()
  drive = GoogleDrive(gauth)
  print("You are logged into Google Drive and are good to go!")

#@markdown Don't forget to hit `Runtime` -> `Run all` after updating the form.

#@title Run Prediction
display_images = True #@param {type:"boolean"}

import sys
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from Bio import BiopythonDeprecationWarning
warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)
from pathlib import Path
from colabfold.download import download_alphafold_params, default_data_dir
from colabfold.utils import setup_logging
from colabfold.batch import get_queries, run, set_model_type
from colabfold.plot import plot_msa_v2

import os
import numpy as np
try:
  K80_chk = os.popen('nvidia-smi | grep "Tesla K80" | wc -l').read()
except:
  K80_chk = "0"
  pass
if "1" in K80_chk:
  print("WARNING: found GPU Tesla K80: limited to total length < 1000")
  if "TF_FORCE_UNIFIED_MEMORY" in os.environ:
    del os.environ["TF_FORCE_UNIFIED_MEMORY"]
  if "XLA_PYTHON_CLIENT_MEM_FRACTION" in os.environ:
    del os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"]

from colabfold.colabfold import plot_protein
from pathlib import Path
import matplotlib.pyplot as plt

# For some reason we need that to get pdbfixer to import
if use_amber and f"/usr/local/lib/python{python_version}/site-packages/" not in sys.path:
    sys.path.insert(0, f"/usr/local/lib/python{python_version}/site-packages/")

def input_features_callback(input_features):
  if display_images:
    plot_msa_v2(input_features)
    plt.show()
    plt.close()

def prediction_callback(protein_obj, length,
                        prediction_result, input_features, mode):
  model_name, relaxed = mode
  if not relaxed:
    if display_images:
      fig = plot_protein(protein_obj, Ls=length, dpi=150)
      plt.show()
      plt.close()

result_dir = jobname
log_filename = os.path.join(jobname,"log.txt")
setup_logging(Path(log_filename))

queries, is_complex = get_queries(queries_path)
model_type = set_model_type(is_complex, model_type)

if "multimer" in model_type and max_msa is not None:
  use_cluster_profile = False
else:
  use_cluster_profile = True

download_alphafold_params(model_type, Path("."))
results = run(
    queries=queries,
    result_dir=result_dir,
    use_templates=use_templates,
    custom_template_path=custom_template_path,
    num_relax=num_relax,
    msa_mode=msa_mode,
    model_type=model_type,
    num_models=5,
    num_recycles=num_recycles,
    relax_max_iterations=relax_max_iterations,
    recycle_early_stop_tolerance=recycle_early_stop_tolerance,
    num_seeds=num_seeds,
    use_dropout=use_dropout,
    model_order=[1,2,3,4,5],
    is_complex=is_complex,
    data_dir=Path("."),
    keep_existing_results=False,
    rank_by="auto",
    pair_mode=pair_mode,
    pairing_strategy=pairing_strategy,
    stop_at_score=float(100),
    prediction_callback=prediction_callback,
    dpi=dpi,
    zip_results=False,
    save_all=save_all,
    max_msa=max_msa,
    use_cluster_profile=use_cluster_profile,
    input_features_callback=input_features_callback,
    save_recycles=save_recycles,
    user_agent="colabfold/google-colab-main",
    calc_extra_ptm=calc_extra_ptm,
)
results_zip = f"{jobname}.result.zip"
os.system(f"zip -r {results_zip} {jobname}")

#@title Display 3D structure {run: "auto"}
import py3Dmol
import glob
import matplotlib.pyplot as plt
from colabfold.colabfold import plot_plddt_legend
from colabfold.colabfold import pymol_color_list, alphabet_list
rank_num = 1 #@param ["1", "2", "3", "4", "5"] {type:"raw"}
color = "lDDT" #@param ["chain", "lDDT", "rainbow"]
show_sidechains = False #@param {type:"boolean"}
show_mainchains = False #@param {type:"boolean"}

tag = results["rank"][0][rank_num - 1]
jobname_prefix = ".custom" if msa_mode == "custom" else ""
pdb_filename = f"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb"
pdb_file = glob.glob(pdb_filename)

def show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color="lDDT"):
  model_name = f"rank_{rank_num}"
  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)
  view.addModel(open(pdb_file[0],'r').read(),'pdb')

  if color == "lDDT":
    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})
  elif color == "rainbow":
    view.setStyle({'cartoon': {'color':'spectrum'}})
  elif color == "chain":
    chains = len(queries[0][1]) + 1 if is_complex else 1
    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):
       view.setStyle({'chain':chain},{'cartoon': {'color':color}})

  if show_sidechains:
    BB = ['C','O','N']
    view.addStyle({'and':[{'resn':["GLY","PRO"],'invert':True},{'atom':BB,'invert':True}]},
                        {'stick':{'colorscheme':f"WhiteCarbon",'radius':0.3}})
    view.addStyle({'and':[{'resn':"GLY"},{'atom':'CA'}]},
                        {'sphere':{'colorscheme':f"WhiteCarbon",'radius':0.3}})
    view.addStyle({'and':[{'resn':"PRO"},{'atom':['C','O'],'invert':True}]},
                        {'stick':{'colorscheme':f"WhiteCarbon",'radius':0.3}})
  if show_mainchains:
    BB = ['C','O','N','CA']
    view.addStyle({'atom':BB},{'stick':{'colorscheme':f"WhiteCarbon",'radius':0.3}})

  view.zoomTo()
  return view

show_pdb(rank_num, show_sidechains, show_mainchains, color).show()
if color == "lDDT":
  plot_plddt_legend().show()

#@title Plots {run: "auto"}
from IPython.display import display, HTML
import base64
from html import escape

# see: https://stackoverflow.com/a/53688522
def image_to_data_url(filename):
  ext = filename.split('.')[-1]
  prefix = f'data:image/{ext};base64,'
  with open(filename, 'rb') as f:
    img = f.read()
  return prefix + base64.b64encode(img).decode('utf-8')

pae = ""
pae_file = os.path.join(jobname,f"{jobname}{jobname_prefix}_pae.png")
if os.path.isfile(pae_file):
    pae = image_to_data_url(pae_file)
cov = image_to_data_url(os.path.join(jobname,f"{jobname}{jobname_prefix}_coverage.png"))
plddt = image_to_data_url(os.path.join(jobname,f"{jobname}{jobname_prefix}_plddt.png"))
display(HTML(f"""
<style>
  img {{
    float:left;
  }}
  .full {{
    max-width:100%;
  }}
  .half {{
    max-width:50%;
  }}
  @media (max-width:640px) {{
    .half {{
      max-width:100%;
    }}
  }}
</style>
<div style="max-width:90%; padding:2em;">
  <h1>Plots for {escape(jobname)}</h1>
  { '<!--' if pae == '' else '' }<img src="{pae}" class="full" />{ '-->' if pae == '' else '' }
  <img src="{cov}" class="half" />
  <img src="{plddt}" class="half" />
</div>
"""))

#@title Package and download results
#@markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \"Download\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).

if msa_mode == "custom":
  print("Don't forget to cite your custom MSA generation method.")

files.download(f"{jobname}.result.zip")

if save_to_google_drive == True and drive:
  uploaded = drive.CreateFile({'title': f"{jobname}.result.zip"})
  uploaded.SetContentFile(f"{jobname}.result.zip")
  uploaded.Upload()
  print(f"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}")